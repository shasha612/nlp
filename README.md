NLP-project-DSCI-6004
WAlthough Q&A systems have become commonplace in many businesses, the legal domain has proven challenging because of the abundance of specialised information. Retrieval-based methods have demonstrated potential as more substantial pretrained language models become available. Creating a retrieval-based system to answer medical questions is the aim of this research. We do this by extending our knowledge using massive language models and graphs. First, we efficiently obtain a huge yet coarse set of replies using Elasticsearch. Next, we integrate semantic matching with pretrained language models to obtain a fine-tuned ranking, leveraging knowledge graphs and named entity recognition to leverage the relationship between the entities in the query and answer. We provide an extensive analysis of the legal reading comprehension test as well as this dataset.

#Method

This project's aim is to develop a question answering system in the legal domain. For the implementation of a T5-based model which returns “an answer,” given a user question and a passage which includes the answer to the question. For this question answering task, we used CUAD dataset tuning a model that is trained on SQUAD 2.0. Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.Contract Understanding Atticus Dataset (CUAD) v1 is a corpus of 13,000+ labels in 510 commercial legal contracts that have been manually labeled under the supervision of experienced lawyers to identify 41 types of legal clauses that are considered important in contact review in connection with a corporate transaction, including mergers & acquisitions, etc.. I started with the t5-base pretrained model “t5- base-uncased” and fine-tuned it to have a question answering task. For Question Answering we use the t5 for Question Answering class from the transformer's library. The T5 model is a transformer-based neural network architecture. Transformer models are known for their parallelizable and scalable architecture, making them suitable for a wide range of NLP tasks..T5 is pre-trained on a large corpus of text data using a denoising autoencoder objective. It learns to generate the target text from a corrupted or noisy version of the same text.The model is then fine-tuned on specific downstream tasks by providing task-specific input-output pairs.. The Transformers library has a diverse collection of pre-trained models which we can reference by name and load easily. The vocabulary of this model is identical to the one in t5-base-uncased. The T5 tokenizer is responsible for converting raw text into a format suitable for input to the T5 model. It breaks down the input text into smaller units called tokens
